{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Train Model\n",
    "#### Note: If you are not retraining the model, you will just need to execute the Score Notebook on the refreshed data.  If you are retraining the model, ensure that the version number is updated prior to serializing the model to disk to version models over time for comparisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all required modules including Oracle connection and  Data Processing Functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T18:35:13.927205Z",
     "start_time": "2019-12-20T18:35:07.717311Z"
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import cx_Oracle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import statsmodels.api as sm\n",
    "import joblib\n",
    "import datetime\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from statistics import mean\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, SVMSMOTE, KMeansSMOTE\n",
    "\n",
    "# Update path to where function file resides\n",
    "if os.name == 'nt':\n",
    "    state = !cd\n",
    "    \n",
    "    # Load DB Connection File from Windows Machine\n",
    "    os.chdir(r'directory name')\n",
    "    from db_connection import oracle_connection\n",
    "    \n",
    "    # Load function file from Windows Machine\n",
    "    os.chdir(r'directory name')\n",
    "    from general_functions import *\n",
    "elif os.name == 'posix':\n",
    "    state = !pwd\n",
    "    \n",
    "    # Load DB Connection File from Mac Machine\n",
    "    os.chdir('directory name')\n",
    "    from db_connection import oracle_connection\n",
    "    \n",
    "    # Load function file from Mac Machine\n",
    "    os.chdir('directory name')\n",
    "    from general_functions import *\n",
    "else:\n",
    "    print('No OS!')\n",
    "\n",
    "#Change directory back to working Jupyter Notebook Directory after importing connection module\n",
    "os.chdir(state[0])\n",
    "\n",
    "todays_date = datetime.date.today().strftime('%Y%m%d')\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DB Connection String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T18:35:16.397666Z",
     "start_time": "2019-12-20T18:35:13.930482Z"
    }
   },
   "outputs": [],
   "source": [
    "if os.name == 'nt':\n",
    "    # Update path to where config file resides\n",
    "    db_creds = os.path.expanduser('~') + 'directory name'\n",
    "    creds = oracle_connection(db_creds)\n",
    "\n",
    "    url = creds['host'] + \":\" + creds['port'] + \"/\" + creds['database']\n",
    "\n",
    "    db = cx_Oracle.connect(creds['user'], creds['password'], url)\n",
    "\n",
    "    cursor = db.cursor()\n",
    "elif os.name == 'posix':\n",
    "    # Update path to where config file resides\n",
    "    db_creds = os.path.expanduser('~') + 'directory name'\n",
    "    creds = oracle_connection(db_creds)\n",
    "\n",
    "    url = creds['host'] + \":\" + creds['port'] + \"/\" + creds['database']\n",
    "\n",
    "    db = cx_Oracle.connect(creds['user'], creds['password'], url, encoding = 'UTF-8')\n",
    "    cursor = db.cursor()\n",
    "else:\n",
    "    print('No OS!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send query to Oracle database and return as Pandas DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T18:37:37.880783Z",
     "start_time": "2019-12-20T18:35:16.402455Z"
    }
   },
   "outputs": [],
   "source": [
    "# Rerun the train sample to include the new data sources\n",
    "\n",
    "query = \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "df = pd.read_sql(query, cursor.connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T18:37:37.894311Z",
     "start_time": "2019-12-20T18:37:37.883745Z"
    }
   },
   "outputs": [],
   "source": [
    "LABEL_VAL = 'column'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T18:37:38.475246Z",
     "start_time": "2019-12-20T18:37:37.898635Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T18:37:38.488286Z",
     "start_time": "2019-12-20T18:37:38.478779Z"
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T18:38:16.172337Z",
     "start_time": "2019-12-20T18:38:15.412039Z"
    }
   },
   "outputs": [],
   "source": [
    "df_tr = df.copy()\n",
    "df_tr = df_tr.drop(['column'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace all missing values with 'None' or 0 depending on the Data Type of column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T18:38:21.907906Z",
     "start_time": "2019-12-20T18:38:16.685994Z"
    }
   },
   "outputs": [],
   "source": [
    "dtype_dict_value = replace_values(df_tr, char_value = 'Unknown')\n",
    "\n",
    "df_tr.fillna(value = dtype_dict_value, inplace = True)\n",
    "df_tr.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data into correct buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T18:38:30.014964Z",
     "start_time": "2019-12-20T18:38:21.911361Z"
    }
   },
   "outputs": [],
   "source": [
    "df_tr['column'] = df_tr['column'].str.replace('_', ' ')\n",
    "\n",
    "df_tr.loc[(df_tr['column'] == 'value'),'column'] = 'value'\n",
    "df_tr.loc[(df_tr['column'] == 'value'),'column'] = 'value'\n",
    "df_tr.loc[(df_tr['column'] == 'value'),'column'] = 'value'\n",
    "df_tr.loc[(df_tr['column'] == 'value'),'column'] = 'valule'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert all columns that are Factor Levels or Flag columns into Category data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T18:38:36.657525Z",
     "start_time": "2019-12-20T18:38:30.021124Z"
    }
   },
   "outputs": [],
   "source": [
    "# One Hot Encoding - Extra Columns (Can Pass Values in list)\n",
    "df_tr, forward_mapping_dict, inv_mapping_dict, encoder = convert_cat_to_cat_lvl(df_tr, encode_method = 'OneHot')\n",
    "\n",
    "# Numeric Encoding - Inplace (Can Pass Values in List)\n",
    "#df_tr, forward_mapping_dict, inv_mapping_dict, encoder = convert_cat_to_cat_lvl(df_tr, encode_method = 'Numeric')\n",
    "\n",
    "df_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Highly Correlated Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T18:39:39.493105Z",
     "start_time": "2019-12-20T18:38:36.663308Z"
    }
   },
   "outputs": [],
   "source": [
    "corr_df, dtr_unique_corr_cols = corr_vars(df_tr, corr_threshold = .95)\n",
    "%store dtr_unique_corr_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T18:39:39.726558Z",
     "start_time": "2019-12-20T18:39:39.495299Z"
    }
   },
   "outputs": [],
   "source": [
    "df_tr = df_tr.drop(dtr_unique_corr_cols, axis = 1)\n",
    "df_tr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize columns using Scaler Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T18:39:44.068218Z",
     "start_time": "2019-12-20T18:39:39.730044Z"
    }
   },
   "outputs": [],
   "source": [
    "df_std, scaler_fit = standardize_cols(df_tr, LABEL_VAL)\n",
    "\n",
    "df_std.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data into Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T18:39:44.604212Z",
     "start_time": "2019-12-20T18:39:44.070800Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df_std, test_size = 0.33, random_state = 5)\n",
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T18:39:44.925477Z",
     "start_time": "2019-12-20T18:39:44.607769Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_label = train_df[LABEL_VAL]\n",
    "train_features = train_df.drop([LABEL_VAL], axis = 1)\n",
    "\n",
    "test_label = test_df[LABEL_VAL]\n",
    "test_features = test_df.drop([LABEL_VAL], axis = 1)\n",
    "\n",
    "train_label_count = train_label.value_counts()\n",
    "test_label_count = test_label.value_counts()\n",
    "\n",
    "print(\"Train\\n\", train_label_count, \"\\nTest\\n\", test_label_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use SMOTE to generate a more balanced data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T18:39:46.948641Z",
     "start_time": "2019-12-20T18:39:44.928561Z"
    }
   },
   "outputs": [],
   "source": [
    "# Can pass 'smote', 'borderline', 'svm', 'kmeans' as smote_method\n",
    "over_samp = generate_smote_sample(smote_method = 'smote', n_jobs = 16)\n",
    "\n",
    "over_samp_feat, over_samp_label = over_samp.fit_sample(train_features, train_label)\n",
    "over_samp_columns = train_features.columns\n",
    "\n",
    "over_samp_feat = pd.DataFrame(data = over_samp_feat, columns = over_samp_columns)\n",
    "over_samp_label = pd.DataFrame(data = over_samp_label, columns = [LABEL_VAL])\n",
    "\n",
    "os_train_label = over_samp_label[LABEL_VAL]\n",
    "\n",
    "# we can Check the numbers of our data\n",
    "print(\"Length of oversampled data is \",len(over_samp_feat))\n",
    "print(\"Number of no target products in oversampled data\",len(over_samp_label[over_samp_label[LABEL_VAL] == 0]))\n",
    "print(\"Number of target products\",len(over_samp_label[over_samp_label[LABEL_VAL] == 1]))\n",
    "print(\"Proportion of no target data in oversampled data is \",len(over_samp_label[over_samp_label[LABEL_VAL] == 0])/len(over_samp_feat))\n",
    "print(\"Proportion of target data in oversampled data is \",len(over_samp_label[over_samp_label[LABEL_VAL] == 1])/len(over_samp_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T18:39:46.969080Z",
     "start_time": "2019-12-20T18:39:46.951376Z"
    }
   },
   "outputs": [],
   "source": [
    "os_train_label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model on SMOTE Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T18:53:13.134403Z",
     "start_time": "2019-12-20T18:51:34.155969Z"
    }
   },
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression(solver = 'liblinear', penalty = 'l1', random_state = 5, n_jobs = 4)\n",
    "\n",
    "\n",
    "lr_model_fit, lr_model_summary = generate_model_summary(df_std\n",
    "                                                     , lr_model\n",
    "                                                     , over_samp_feat\n",
    "                                                     , os_train_label\n",
    "                                                     , test_features\n",
    "                                                     , test_label\n",
    "                                                     , is_tree_model = False)\n",
    "\n",
    "lr_model_score = lr_model_summary[0]\n",
    "lr_model_confusion_list = lr_model_summary[1]\n",
    "lr_model_metrics = lr_model_summary[2]\n",
    "lr_fpr = lr_model_summary[3]\n",
    "lr_tpr = lr_model_summary[4]\n",
    "lr_roc_score = lr_model_summary[6]\n",
    "\n",
    "print(\"Model Accuracy Score: \", lr_model_summary[0]\n",
    "      , \"\\nModel Confusion List (TN, FP, FN, TP): \", lr_model_summary[1]\n",
    "      , \"\\nModel Summary: \\n\", lr_model_summary[2]\n",
    "      , \"\\nROC Score: \", lr_model_summary[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T18:53:24.867051Z",
     "start_time": "2019-12-20T18:53:13.137452Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logit_model_os = sm.Logit(os_train_label, over_samp_feat)\n",
    "lr_result_os = logit_model_os.fit(method = 'bfgs')\n",
    "print(lr_result_os.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T18:53:26.575763Z",
     "start_time": "2019-12-20T18:53:24.869768Z"
    }
   },
   "outputs": [],
   "source": [
    "sum_df = lr_result_os.summary2().tables[1]\n",
    "sum_df[\"INFLUENCE\"] = (np.std(over_samp_feat, 0)*sum_df[\"Coef.\"])\n",
    "sum_df.loc[:, (\"Coef.\", \"P>|z|\", \"INFLUENCE\")].sort_values(by = \"INFLUENCE\", axis = 0, ascending = False).head(20)\n",
    "\n",
    "sort_sum_df = sum_df.sort_values(by = \"INFLUENCE\", axis = 0, ascending = False).head(20)\n",
    "dtr_lr_import_feats = sort_sum_df.index.values\n",
    "\n",
    "print(dtr_lr_import_feats)\n",
    "\n",
    "#%store dtr_lr_import_feats - V1 Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest on SMOTE Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TOP Hyper Parameters: n_estimators = 135, max_depth = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T18:57:24.963923Z",
     "start_time": "2019-12-20T18:55:02.485091Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators = 250, max_depth = 30, random_state = 5, n_jobs = -1)\n",
    "\n",
    "rf_model_fit, rf_model_summary = generate_model_summary(df_std\n",
    "                                                     , rf_model\n",
    "                                                     , over_samp_feat\n",
    "                                                     , os_train_label\n",
    "                                                     , test_features\n",
    "                                                     , test_label\n",
    "                                                     , is_tree_model = True)\n",
    "\n",
    "rf_model_score = rf_model_summary[0]\n",
    "rf_model_confusion_list = rf_model_summary[1]\n",
    "rf_model_metrics = rf_model_summary[2]\n",
    "rf_fpr = rf_model_summary[3]\n",
    "rf_tpr = rf_model_summary[4]\n",
    "rf_roc_score = rf_model_summary[6]\n",
    "\n",
    "print(\"Model Accuracy Score: \", rf_model_summary[0]\n",
    "      , \"\\nModel Confusion List (TN, FP, FN, TP): \", rf_model_summary[1]\n",
    "      , \"\\nModel Summary: \\n\", rf_model_summary[2]\n",
    "      , \"\\nROC Score: \", rf_model_summary[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T18:57:25.108218Z",
     "start_time": "2019-12-20T18:57:24.968965Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rf_import_df = pd.DataFrame()\n",
    "rf_import_df['FEATURE_NAME'], rf_import_df['FEATURE_IMPORTANCE'] = train_features.columns, rf_model_fit.feature_importances_\n",
    "rf_import_cols = list(rf_import_df.sort_values(by = ['FEATURE_IMPORTANCE'], axis = 0, ascending = False).head(20)['FEATURE_NAME'])\n",
    "rf_import_df.sort_values(by = ['FEATURE_IMPORTANCE'], axis = 0, ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Model on SMOTE Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T21:21:20.684746Z",
     "start_time": "2019-12-20T20:05:52.287905Z"
    }
   },
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(n_estimators = 1000, max_depth = 6, learning_rate = 0.05, random_state = 5, n_jobs = 4)\n",
    "\n",
    "xgb_model_fit, xgb_model_summary = generate_model_summary(df_std\n",
    "                                                         , xgb_model\n",
    "                                                         #, train_features\n",
    "                                                         #, train_label\n",
    "                                                         , over_samp_feat\n",
    "                                                         , os_train_label\n",
    "                                                         , test_features\n",
    "                                                         , test_label\n",
    "                                                         , is_tree_model = True)\n",
    "\n",
    "xgb_model_score = xgb_model_summary[0]\n",
    "xgb_model_confusion_list = xgb_model_summary[1]\n",
    "xgb_model_metrics = xgb_model_summary[2]\n",
    "xgb_fpr = xgb_model_summary[3]\n",
    "xgb_tpr = xgb_model_summary[4]\n",
    "xgb_roc_score = xgb_model_summary[6]\n",
    "\n",
    "print(\"Model Accuracy Score: \", xgb_model_summary[0]\n",
    "      , \"\\nModel Confusion List (TN, FP, FN, TP): \", xgb_model_summary[1]\n",
    "      , \"\\nModel Summary: \\n\", xgb_model_summary[2]\n",
    "      , \"\\nROC Score: \", xgb_model_summary[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T21:21:20.832558Z",
     "start_time": "2019-12-20T21:21:20.686945Z"
    }
   },
   "outputs": [],
   "source": [
    "xgb_import_df = pd.DataFrame()\n",
    "xgb_import_df['FEATURE_NAME'], xgb_import_df['FEATURE_IMPORTANCE'] = train_features.columns, xgb_model_fit.feature_importances_\n",
    "xgb_import_cols = list(xgb_import_df.sort_values(by = ['FEATURE_IMPORTANCE'], axis = 0, ascending = False).head(30)['FEATURE_NAME'])\n",
    "xgb_import_df.sort_values(by = ['FEATURE_IMPORTANCE'], axis = 0, ascending = False).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare results of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T21:22:57.570811Z",
     "start_time": "2019-12-20T21:22:57.560821Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Logistic Regression SMOTE Score: \" + str(lr_model_score) + \"\\n\"\n",
    "      , \"Random Forest SMOTE Score: \" + str(rf_model_score) + \"\\n\"\n",
    "      , \"XGBoost SMOTE Score: \" + str(xgb_model_score) + \"\\n\"\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T21:22:58.035960Z",
     "start_time": "2019-12-20T21:22:58.029291Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix: \\nTrue Negative, False Positive, False Negative, True Positive \\n\\n\"\n",
    "      , \"Logistic Regression SMOTE: \" + str(lr_model_confusion_list) + \"\\n\"\n",
    "      , \"Random Forest SMOTE: \" + str(rf_model_confusion_list) + \"\\n\"\n",
    "      , \"XGBoost SMOTE: \" + str(xgb_model_confusion_list) + \"\\n\"\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T21:22:58.554936Z",
     "start_time": "2019-12-20T21:22:58.547176Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nLogistic Regression SMOTE Model Metrics: \\n\", lr_model_metrics\n",
    "      , \"\\nRandom Forest SMOTE Model Metrics: \\n\", rf_model_metrics\n",
    "      , \"\\nXGBoost SMOTE Model Metrics: \\n\", xgb_model_metrics\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T21:22:59.259083Z",
     "start_time": "2019-12-20T21:22:59.248431Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Logistic Regression SMOTE ROC Score: \", lr_roc_score\n",
    "      , \"\\nRandom Forest SMOTE ROC Score: \", rf_roc_score\n",
    "      , \"\\nXGBoost SMOTE ROC Score: \", xgb_roc_score\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T21:23:00.492558Z",
     "start_time": "2019-12-20T21:23:00.027113Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (20,18))\n",
    "plt.plot(lr_fpr, lr_tpr, color = 'red', lw = 2, label = 'LR ROC Curve (area = %0.2f)' %lr_roc_score)\n",
    "plt.plot(rf_fpr, rf_tpr, color = 'blue', lw = 2, label = 'RF ROC Curve (area = %0.2f)' %rf_roc_score)\n",
    "plt.plot(xgb_fpr, xgb_tpr, color = 'green', lw = 2, label = 'XGBoost ROC Curve (area = %0.2f)' %xgb_roc_score)\n",
    "plt.plot([0, 1], [0, 1], color = 'black', lw = 2, linestyle = '--')\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serialize the data model for future use\n",
    "#### Update version number if you are retraining the model.  USe versioning on serialized models for future comparisions of model effectiveness over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T21:23:04.211615Z",
     "start_time": "2019-12-20T21:23:03.851187Z"
    }
   },
   "outputs": [],
   "source": [
    "joblib.dump(lr_model_fit, './Model/dtr_logistic_regression_v2.0.pkl')\n",
    "joblib.dump(rf_model_fit, './Model/dtr_random_forest_v2.0.pkl')\n",
    "joblib.dump(xgb_model_fit, './Model/dtr_xgboost_v2.0.pkl')\n",
    "joblib.dump(encoder, './Model/encoder_v1.0.pkl')\n",
    "joblib.dump(scaler_fit, './Model/scaler_v1.0.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T21:23:09.541074Z",
     "start_time": "2019-12-20T21:23:07.770164Z"
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "         \n",
    "        \"\"\"\n",
    "\n",
    "valid_df = pd.read_sql(query, cursor.connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T21:23:10.479151Z",
     "start_time": "2019-12-20T21:23:10.059675Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T21:23:12.637901Z",
     "start_time": "2019-12-20T21:23:12.627677Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_df_tr = valid_df.copy()\n",
    "valid_df_tr = valid_df_tr.drop(['column'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace all missing values with 'None' or 0 depending on the Data Type of column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T21:23:13.857597Z",
     "start_time": "2019-12-20T21:23:13.689379Z"
    }
   },
   "outputs": [],
   "source": [
    "#dtype_dict_value = replace_values(valid_df_tr)\n",
    "valid_df_tr = valid_df_tr.fillna(value = dtype_dict_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T21:23:44.744769Z",
     "start_time": "2019-12-20T21:23:44.732459Z"
    }
   },
   "outputs": [],
   "source": [
    "string_col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T21:24:39.880329Z",
     "start_time": "2019-12-20T21:24:39.624780Z"
    }
   },
   "outputs": [],
   "source": [
    "string_col_list = list(df.select_dtypes(include = ['object']).columns)\n",
    "string_col_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert all columns that are Factor Levels or Flag columns into Category data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T21:23:17.168845Z",
     "start_time": "2019-12-20T21:23:15.612277Z"
    }
   },
   "outputs": [],
   "source": [
    "string_col_list = list(valid_df_tr.select_dtypes(include = ['object']).columns)\n",
    "encode_df = pd.DataFrame(encoder.transform(valid_df_tr[string_col_list]).toarray(), columns = encoder.get_feature_names(string_col_list))\n",
    "\n",
    "encode_col_dict = create_encode_col_dict(valid_df_tr, encoder)\n",
    "\n",
    "valid_df_tr = valid_df_tr.merge(encode_df, left_index = True, right_index = True)\n",
    "valid_df_tr = valid_df_tr.drop(string_col_list,  axis = 1)\n",
    "\n",
    "valid_df_tr = valid_df_tr.rename(columns = encode_col_dict)\n",
    "\n",
    "valid_df_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Highly Correlated Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T14:28:47.440952Z",
     "start_time": "2019-12-20T14:28:47.420660Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid_df_tr = valid_df_tr.drop(dtr_unique_corr_cols, axis = 1)\n",
    "valid_df_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T14:28:48.563815Z",
     "start_time": "2019-12-20T14:28:48.243025Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_df_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize columns using Scaler Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T14:28:50.639154Z",
     "start_time": "2019-12-20T14:28:50.619120Z"
    }
   },
   "outputs": [],
   "source": [
    "label = valid_df_tr[LABEL_VAL]\n",
    "column_headers = valid_df_tr.drop(LABEL_VAL, axis = 1).columns\n",
    "valid_df_std = pd.DataFrame(scaler_fit.transform(valid_df_tr.drop(LABEL_VAL, axis = 1)), columns = column_headers)\n",
    "valid_df_std = pd.DataFrame(label).merge(valid_df_std, left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T14:28:54.306476Z",
     "start_time": "2019-12-20T14:28:53.950810Z"
    }
   },
   "outputs": [],
   "source": [
    "dtr_xgb_col_order = list(over_samp_feat.columns)\n",
    "\n",
    "%store dtr_xgb_col_order\n",
    "\n",
    "xgb_features = valid_df_std.reindex(columns = dtr_xgb_col_order)\n",
    "\n",
    "xgb_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create probability and scored df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T14:28:55.880319Z",
     "start_time": "2019-12-20T14:28:55.750156Z"
    }
   },
   "outputs": [],
   "source": [
    "features = valid_df_std.drop([LABEL_VAL], axis = 1)\n",
    "valid_df_std['PRED_LABEL_LR'] = lr_model_fit.predict(features)\n",
    "valid_df_std['PRED_LABEL_RF'] = rf_model_fit.predict(features)\n",
    "valid_df_std['PRED_LABEL_XGB'] = xgb_model_fit.predict(xgb_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T14:28:57.123503Z",
     "start_time": "2019-12-20T14:28:56.637403Z"
    }
   },
   "outputs": [],
   "source": [
    "prob_df = pd.DataFrame()\n",
    "\n",
    "prob_df['LR_PROB_ZERO'] = lr_model_fit.predict_proba(features)[: ,0]\n",
    "prob_df['LR_PROB_ONE'] = lr_model_fit.predict_proba(features)[: ,1]\n",
    "prob_df['RF_PROB_ZERO'] = rf_model_fit.predict_proba(features)[: ,0]\n",
    "prob_df['RF_PROB_ONE'] = rf_model_fit.predict_proba(features)[: ,1]\n",
    "prob_df['XGB_PROB_ZERO'] = xgb_model_fit.predict_proba(xgb_features)[:, 0]\n",
    "prob_df['XGB_PROB_ONE'] = xgb_model_fit.predict_proba(xgb_features)[:, 1]\n",
    "\n",
    "valid_df_std = valid_df_std.join(prob_df)\n",
    "#valid_df_std.rename({0: 'PROB_ZERO', 1: 'PROB_ONE'}, axis = 1, inplace = True)\n",
    "valid_df_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T14:28:58.018641Z",
     "start_time": "2019-12-20T14:28:57.979216Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid_df_std[\"AVG_PROB_ZERO\"] = (valid_df_std[\"LR_PROB_ZERO\"] + valid_df_std[\"RF_PROB_ZERO\"])/2\n",
    "valid_df_std[\"AVG_PROB_ONE\"] = (valid_df_std[\"LR_PROB_ONE\"] + valid_df_std[\"RF_PROB_ONE\"])/2\n",
    "\n",
    "valid_df_std[['DATORAMA_FLG'\n",
    "              , 'PRED_LABEL_LR'\n",
    "              , 'LR_PROB_ZERO'\n",
    "              , 'LR_PROB_ONE'\n",
    "              , 'PRED_LABEL_RF'\n",
    "              , 'RF_PROB_ZERO'\n",
    "              , 'RF_PROB_ONE'\n",
    "              , 'PRED_LABEL_XGB'\n",
    "              , 'XGB_PROB_ZERO'\n",
    "              , 'XGB_PROB_ONE'\n",
    "              , 'AVG_PROB_ZERO'\n",
    "              , 'AVG_PROB_ONE']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deprecated Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:00:36.620311Z",
     "start_time": "2019-09-05T14:00:35.608717Z"
    }
   },
   "outputs": [],
   "source": [
    "# Deprecated\n",
    "\n",
    "#dtr_low_mean_col_list = low_mean_cols(df_tr, .001)\n",
    "#print(dtr_low_mean_col_list)\n",
    "#df_tr = df_tr.drop(dtr_low_mean_col_list, axis = 1)\n",
    "\n",
    "#%store dtr_low_mean_col_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove all potential outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:00:42.801280Z",
     "start_time": "2019-09-05T14:00:42.019094Z"
    }
   },
   "outputs": [],
   "source": [
    "# Deprecated\n",
    "\n",
    "#outlier_idx_list = potential_outliers(df_tr)\n",
    "#print(outlier_idx_list)\n",
    "\n",
    "#df_tr = df_tr.drop(outlier_idx_list, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Only scaling the non flag columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:00:47.343417Z",
     "start_time": "2019-09-05T14:00:47.329208Z"
    }
   },
   "outputs": [],
   "source": [
    "# Deprecated\n",
    "\n",
    "# Example for how to pass specific values to be standardized\n",
    "\n",
    "#column_headers = list(df_tr.columns)\n",
    "#flag_col_list = [col for col in column_headers if '_FLG' in col]\n",
    "#column_headers = [col for col in column_headers if col not in flag_col_list]\n",
    "\n",
    "#df_std, scaler_fit = standardize_cols(df_tr, 'DATORAMA_FLG', column_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run K-fold - V1 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:02:40.244476Z",
     "start_time": "2019-09-05T14:02:40.234886Z"
    }
   },
   "outputs": [],
   "source": [
    "#k_fold_rec, k_fold_prec, k_fold_f_score = run_k_fold(df_std, 'DATORAMA_FLG', lr_model, 2)\n",
    "#print(k_fold_rec, k_fold_prec, k_fold_f_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scale only on the non flag columns in DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:03:32.330365Z",
     "start_time": "2019-09-05T14:03:32.320954Z"
    }
   },
   "outputs": [],
   "source": [
    "# Deprecated\n",
    "#column_headers = list(valid_df_tr.columns)\n",
    "#flag_col_list = [col for col in column_headers if '_FLG' in col]\n",
    "#column_headers = [col for col in column_headers if col not in flag_col_list]\n",
    "\n",
    "#valid_df_std, valid_scaler_fit = standardize_cols(valid_df_tr, 'DATORAMA_FLG', column_headers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
