{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eDataSource Fuzzy Match Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T12:33:54.086591Z",
     "start_time": "2020-08-25T12:33:54.030396Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import cx_Oracle\n",
    "import time\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "#from fuzzywuzzy import fuzz, process\n",
    "from dask import dataframe as dd, delayed, compute, distributed\n",
    "from dask.multiprocessing import get\n",
    "import multiprocess as mp\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "## Test out preformance improvement over FuzzyWuzzy\n",
    "from rapidfuzz import fuzz, process\n",
    "\n",
    "\n",
    "# Update path to where function file resides\n",
    "if os.name == 'nt':\n",
    "    state = !cd\n",
    "    \n",
    "    # Load DB Connection File from Windows Machine\n",
    "    os.chdir(r'Directory Name')\n",
    "    from db_connection import oracle_connection\n",
    "\n",
    "elif os.name == 'posix':\n",
    "    state = !pwd\n",
    "    \n",
    "    # Load DB Connection File from Mac Machine\n",
    "    os.chdir('Directory Name')\n",
    "    from db_connection import oracle_connection\n",
    "\n",
    "else:\n",
    "    print('No OS!')\n",
    "\n",
    "#Change directory back to working Jupyter Notebook Directory after importing connection module\n",
    "os.chdir(state[0])\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DB Connection String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T12:33:56.497822Z",
     "start_time": "2020-08-25T12:33:55.889222Z"
    }
   },
   "outputs": [],
   "source": [
    "if os.name == 'nt':\n",
    "    # Update path to where config file resides\n",
    "    db_creds = os.path.expanduser('~') + 'Directory Name'\n",
    "    creds = oracle_connection(db_creds)\n",
    "\n",
    "    url = creds['host'] + \":\" + creds['port'] + \"/\" + creds['database']\n",
    "\n",
    "    db = cx_Oracle.connect(creds['user'], creds['password'], url, encoding = 'UTF-8')\n",
    "\n",
    "    cursor = db.cursor()\n",
    "elif os.name == 'posix':\n",
    "    # Update path to where config file resides\n",
    "    db_creds = os.path.expanduser('~') + 'Directory Name'\n",
    "    creds = oracle_connection(db_creds)\n",
    "\n",
    "    url = creds['host'] + \":\" + creds['port'] + \"/\" + creds['database']\n",
    "\n",
    "    db = cx_Oracle.connect(creds['user'], creds['password'], url, encoding = 'UTF-8')\n",
    "    cursor = db.cursor()\n",
    "else:\n",
    "    print('No OS!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the CSV files for processing.\n",
    "#### NOTE: This process may be automated via API access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T12:34:48.937934Z",
     "start_time": "2020-08-25T12:33:58.482165Z"
    }
   },
   "outputs": [],
   "source": [
    "FILE_PATH = r'Directory Name'\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for filename in os.listdir(FILE_PATH):\n",
    "    if filename.endswith('.csv'):\n",
    "        tmp_df = pd.read_csv(FILE_PATH + filename, keep_default_na = False, encoding = 'ANSI')\n",
    "        df_list.append(tmp_df)\n",
    "\n",
    "df = pd.concat(df_list, axis = 0, ignore_index = False, sort = False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T12:34:57.495846Z",
     "start_time": "2020-08-25T12:34:48.939931Z"
    }
   },
   "outputs": [],
   "source": [
    "## Add additional columns to enhance the fuzzy match logic\n",
    "\n",
    "df['column'] = df['column'].astype(str)\n",
    "\n",
    "df['column'] = df['column'].fillna('-') + ' ' + df['column'].fillna('-') + ' ' + df['column'].fillna('-') + ' ' + df['column'].fillna('-')\n",
    "df['column'] = df['column'].fillna('-') + ' ' + df['column'].fillna('-') + ' ' + df['column'].fillna('-') + ' ' + df['column'].fillna('-') + ' ' + df['column'].fillna('-')\n",
    "\n",
    "sub_df = df[['COLUMN NAME']]\n",
    "\n",
    "agg_df = sub_df.groupby(['COLUMN NAME'], as_index = False)[['COLUMN NAME']].agg('sum')\n",
    "\n",
    "agg_df = agg_df.drop(index = agg_df[agg_df['COLUMN NAME'] == ''].index)\n",
    "\n",
    "esp_df = agg_df.groupby(['column'], as_index = False)[['column']].quantile(q = 0.75)\n",
    "esp_df = esp_df.drop(esp_df.index[0]).reset_index(drop = True)\n",
    "esp_df.rename(columns = {'column': 'column'}, inplace = True)\n",
    "\n",
    "agg_df = pd.merge(agg_df, esp_df, left_on = 'column', right_on = 'column', how = 'left')\n",
    "agg_df = agg_df[(agg_df['column'] >= agg_df['column']) | (agg_df['column'] == '')]\n",
    "agg_df = agg_df.drop(['column'], axis = 1).reset_index(drop = True)\n",
    "\n",
    "agg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T19:09:01.627251Z",
     "start_time": "2020-08-24T19:09:01.618275Z"
    }
   },
   "outputs": [],
   "source": [
    "agg_df['column'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in list of accounts from System for fuzzy wuzzy matchin process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T12:37:22.158807Z",
     "start_time": "2020-08-25T12:34:57.499835Z"
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "org_df = pd.read_sql(query, cursor.connection)\n",
    "org_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Dask for parallel computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T19:27:01.107421Z",
     "start_time": "2020-08-21T19:27:00.513798Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the list for ACCT/ADDR compare\n",
    "org_acct_list = org_df['column'].astype(str)\n",
    "agg_df['column'] = agg_df['column'].astype(str)\n",
    "\n",
    "# Create the list for the ACCT/ADDR/URL compare\n",
    "org_acct_url_list = org_df['column'].astype(str)\n",
    "agg_df['column'] = agg_df['column'].astype(str)\n",
    "\n",
    "acct_id_list = org6df['column']\n",
    "\n",
    "# Create the dictionaries to return the correct account ID\n",
    "org_acct_id_dict = dict(zip(org_acct_list.index, acct_id_list))\n",
    "org_acct_url_id_dict = dict(zip(org_acct_url_list.index, acct_id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T19:27:03.273759Z",
     "start_time": "2020-08-21T19:27:01.111410Z"
    }
   },
   "outputs": [],
   "source": [
    "num_cpu = mp.cpu_count()\n",
    "\n",
    "dask_df = dd.from_pandas(agg_df, npartitions = num_cpu)\n",
    "\n",
    "client = distributed.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T19:27:03.287944Z",
     "start_time": "2020-08-21T19:27:03.273759Z"
    }
   },
   "outputs": [],
   "source": [
    "## May look to improve the process using the RapidFuzz module over FuzzyWuzzy\n",
    "\n",
    "def fuzzy_match_top_list_dask(value_to_compare, list_to_compare_against, dictionary_with_ids, cutoff_thresh):\n",
    "        \n",
    "        value_list = []\n",
    "    \n",
    "        value = process.extractOne(value_to_compare, list_to_compare_against, scorer = fuzz.ratio)\n",
    "\n",
    "        if value[1] >= cutoff_thresh:\n",
    "            (comp_value, match_pct, record_index) = value\n",
    "\n",
    "            rec_id = dictionary_with_ids.get(record_index)\n",
    "            \n",
    "            value_list.extend([comp_value, match_pct, record_index, rec_id])\n",
    "\n",
    "        else:\n",
    "            value_list.extend(['No Value', -1, -1, 'No Acct ID'])\n",
    "\n",
    "        return(value_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T19:57:18.610344Z",
     "start_time": "2020-08-21T19:27:03.292930Z"
    }
   },
   "outputs": [],
   "source": [
    "dask_df['fuzzy_match_id_acct_addr'] = dask_df.map_partitions(\n",
    "    lambda df: df.apply(\n",
    "        lambda x: fuzzy_match_top_list_dask(x.NAME_ADDR_STRING, org_acct_list, org_acct_id_dict, 85), axis = 1\n",
    "    )\n",
    ").compute(scheduler = 'processes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T20:40:10.615032Z",
     "start_time": "2020-08-21T19:57:18.615331Z"
    }
   },
   "outputs": [],
   "source": [
    "dask_df['fuzzy_match_id_acct_addr_url'] = dask_df.map_partitions(\n",
    "    lambda df: df.apply(\n",
    "        lambda x: fuzzy_match_top_list_dask(x.NAME_ADDR_URL_STRING, org_acct_url_list, org_acct_url_id_dict, 85), axis = 1\n",
    "    )\n",
    ").compute(scheduler = 'processes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T20:40:11.661369Z",
     "start_time": "2020-08-21T20:40:10.615032Z"
    }
   },
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T20:40:11.711599Z",
     "start_time": "2020-08-21T20:40:11.665357Z"
    }
   },
   "outputs": [],
   "source": [
    "dask_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T20:40:11.835644Z",
     "start_time": "2020-08-21T20:40:11.712596Z"
    }
   },
   "outputs": [],
   "source": [
    "pd_df = dask_df.compute()\n",
    "pd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T20:40:12.045777Z",
     "start_time": "2020-08-21T20:40:11.836642Z"
    }
   },
   "outputs": [],
   "source": [
    "full_match_db_df = dask_df[['COLUMN NAME']].compute()\n",
    "full_match_db_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T20:40:12.111925Z",
     "start_time": "2020-08-21T20:40:12.047771Z"
    }
   },
   "outputs": [],
   "source": [
    "full_match_db_df[['COLUMN NAME']] = pd.DataFrame(full_match_db_df['fuzzy_match_id_acct_addr'].tolist(), index=full_match_db_df.index)\n",
    "full_match_db_df[['COLUMN NAME']] = pd.DataFrame(full_match_db_df['fuzzy_match_id_acct_addr_url'].tolist(), index=full_match_db_df.index)\n",
    "\n",
    "full_match_db_df = full_match_db_df[['COLUMN NAME']]\n",
    "\n",
    "full_match_db_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop, Recreate table, and insert matched data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T20:40:16.961476Z",
     "start_time": "2020-08-21T20:40:12.115918Z"
    }
   },
   "outputs": [],
   "source": [
    "drop_table_sql = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(drop_table_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T20:40:18.133236Z",
     "start_time": "2020-08-21T20:40:16.965466Z"
    }
   },
   "outputs": [],
   "source": [
    "create_table_sql = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(create_table_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T20:40:18.764761Z",
     "start_time": "2020-08-21T20:40:18.137225Z"
    }
   },
   "outputs": [],
   "source": [
    "records = [tuple(x) for x in full_match_db_df.values]\n",
    "\n",
    "cursor.executemany('''''', records)\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run basic comparison on percentage score to get correct ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T20:40:18.967130Z",
     "start_time": "2020-08-21T20:40:18.769743Z"
    }
   },
   "outputs": [],
   "source": [
    "drop_table_sql = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(drop_table_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T21:27:31.635739Z",
     "start_time": "2020-08-21T20:40:18.971121Z"
    }
   },
   "outputs": [],
   "source": [
    "create_table_sql = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(create_table_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of accounts and data and create aggregation of all data names and usage for an account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T13:59:51.777986Z",
     "start_time": "2020-08-24T13:59:51.209641Z"
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(query, cursor.connection)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T13:59:53.328316Z",
     "start_time": "2020-08-24T13:59:53.305644Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(r'DIRECTORY NAME', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T17:25:03.862690Z",
     "start_time": "2020-08-24T17:25:01.978371Z"
    }
   },
   "outputs": [],
   "source": [
    "acct_nm_list = list(df['column'].unique())\n",
    "\n",
    "tmp_output_df_list = []\n",
    "\n",
    "for acct in acct_nm_list:\n",
    "    tmp_output_df = pd.DataFrame()\n",
    "    tmp_df = df[df['column'] == acct]\n",
    "    \n",
    "    esp_names = tmp_df['column'].to_list()\n",
    "    esp_volume = tmp_df['column'].to_list()\n",
    "    esp_volume = [str(format(val, \",\")) for val in esp_volume]\n",
    "    \n",
    "    combo_list = ['Overall ESP Volume for ' + ': '.join(val) for val in zip(esp_names, esp_volume)]\n",
    "    \n",
    "    oppty_sub_desc = '- ' + '\\n- '.join(combo_list)\n",
    "    \n",
    "    tmp_output_df['column'] = tmp_df['column'].unique()\n",
    "    tmp_output_df['column'] = tmp_df['column'].unique()\n",
    "    tmp_output_df['column'] = tmp_df['column'].unique()\n",
    "    tmp_output_df['column'] = oppty_sub_desc\n",
    "    \n",
    "    tmp_output_df_list.append(tmp_output_df)\n",
    "    \n",
    "output_df = pd.concat(tmp_output_df_list)\n",
    "output_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data aggregated at account level back into DB for last processing step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T17:25:14.072269Z",
     "start_time": "2020-08-24T17:25:12.931103Z"
    }
   },
   "outputs": [],
   "source": [
    "drop_table_sql = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(drop_table_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T17:25:14.210663Z",
     "start_time": "2020-08-24T17:25:14.077256Z"
    }
   },
   "outputs": [],
   "source": [
    "create_table_sql = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(create_table_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T17:25:16.490964Z",
     "start_time": "2020-08-24T17:25:16.301258Z"
    }
   },
   "outputs": [],
   "source": [
    "records = [tuple(x) for x in output_df.values]\n",
    "\n",
    "cursor.executemany('''''', records)\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create final target list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T17:30:48.315024Z",
     "start_time": "2020-08-24T17:30:48.096593Z"
    }
   },
   "outputs": [],
   "source": [
    "drop_table_sql = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(drop_table_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T17:30:50.352919Z",
     "start_time": "2020-08-24T17:30:48.321009Z"
    }
   },
   "outputs": [],
   "source": [
    "create_table_sql = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(create_table_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-24T17:30:52.753757Z",
     "start_time": "2020-08-24T17:30:50.356909Z"
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "final_df = pd.read_sql(query, cursor.connection)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = term (word)\n",
    "# d = document (set of words)\n",
    "# N = count of corpus\n",
    "# corpus = the total document set\n",
    "\n",
    "# Term Frequency (TF) * Inverse Document Frequecny (IDF)\n",
    "# Number of times a word is in a document * the total number of documents the word appears in\n",
    "# TF is individual to each document and word\n",
    "# tf(t, d) = count of t in d / number of words in d\n",
    "\n",
    "# DF is the count of occurrences of term t in the document set N.\n",
    "# df(t) = occurence of t in documents\n",
    "# idf(t) = log(N/df + 1)\n",
    "\n",
    "# tf-idf(t, d) = tf(t, d) * log(n/df+1)\n",
    "\n",
    "# In this problem the documents are the edata company name list and the org62 account name list\n",
    "# the terms are the names in the respective list\n",
    "# We want to identify the most similar org62 account name to that of the edata company names\n",
    "\n",
    "# Documents are each respective account/company name\n",
    "# Words or terms are the words in an account/company name\n",
    "# Corpus is the entire list of company names or account names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T19:20:01.924246Z",
     "start_time": "2020-08-25T19:19:57.439625Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "### User for straight name match\n",
    "company_names_list = list(agg_df['column'].astype(str))\n",
    "org_acct_names_list = list(org_df['column'].astype(str))\n",
    "org_acct_names = org_df['column'].astype(str)\n",
    "\n",
    "acct_id_list = org_df['column']\n",
    "\n",
    "org_acct_id_dict = dict(zip(org_acct_names.index, acct_id_list))\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = ['llc', 'inc', 'corp', 'main', 'dupe'])\n",
    "\n",
    "# Rows in matrices represent the documents (Company Names)\n",
    "# Columns represent unique tokens (or words)\n",
    "org_mat = vectorizer.fit_transform(org_acct_names_list)\n",
    "edata_mat = vectorizer.transform(edata_company_names_list)\n",
    "#print(vectorizer.get_feature_names())\n",
    "\n",
    "cos_sim_mat = cosine_similarity(org_mat, edata_mat)\n",
    "\n",
    "output_list = []\n",
    "\n",
    "for idx, cos_sim in enumerate(cos_sim_mat):\n",
    "    cos_sim = cos_sim.flatten()\n",
    "    target_index = cos_sim.argsort()[-1]\n",
    "    source_index = idx\n",
    "    sim = cos_sim[target_index]\n",
    "    if cos_sim[target_index] >= 0.90:\n",
    "        tmp_df = pd.DataFrame(data = {'column': [org_acct_names_list[source_index]]\n",
    "                                      , 'coluumn': [edata_company_names_list[target_index]]\n",
    "                                      , 'column': [sim]\n",
    "                                      , 'column': [org_acct_id_dict.get(source_index)]\n",
    "                                     }\n",
    "                              , columns = ['COLUMN NAME'])\n",
    "        \n",
    "        output_list.append(tmp_df)\n",
    "        \n",
    "        \n",
    "final_df = pd.concat(output_list)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T19:20:01.932222Z",
     "start_time": "2020-08-25T19:20:01.926239Z"
    }
   },
   "outputs": [],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Target ID for @ Mention in Chatter Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the Target Table with final list of accounts oppty's will be loaded to\n",
    "\n",
    "query = \"\"\"        \n",
    "        \"\"\"\n",
    "\n",
    "manager_df = pd.read_sql(query, cursor.connection)\n",
    "\n",
    "manager_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the Join columns\n",
    "\n",
    "oppty_chatter_df = df.merge(manager_df, how = 'left', left_on = 'column', right_on = 'column', suffixes = (False, False))\n",
    "oppty_chatter_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Chatter Message Body for POST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatter_msg_list = []\n",
    "\n",
    "for idx, val in oppty_chatter_df.iterrows():\n",
    "    oppty_amt = oppty_chatter_df.loc[idx, 'column']\n",
    "    campaign_name = oppty_chatter_df.loc[idx, 'column']\n",
    "    \n",
    "    chatter_msg = ''.format(camp_nm = campaign_name, amt = oppty_amt)\n",
    "\n",
    "    chatter_msg_list.append(chatter_msg)\n",
    "    \n",
    "oppty_chatter_df['column'] = chatter_msg_list\n",
    "    \n",
    "oppty_chatter_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After Successful Load in read file to get ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in glob.glob('directory'):\n",
    "    success_df = pd.read_csv(file)\n",
    "    \n",
    "success_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Column Names\n",
    "\n",
    "oppty_chatter_df = oppty_chatter_df.merge(success_df, how = 'inner', left_on = ['column'], right_on = ['column'], suffixes = ['','_y'])\n",
    "oppty_chatter_df = oppty_chatter_df.drop(oppty_chatter_df.columns.difference(['COLUMN NAME']), axis = 1)\n",
    "oppty_chatter_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push Chatter Message to AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Purposes\n",
    "#oppty_chatter_df = oppty_chatter_df.head(1)\n",
    "#oppty_chatter_df['column'] = ''\n",
    "#oppty_chatter_df['column'] = ''\n",
    "#oppty_chatter_df['column'] = ''\n",
    "\n",
    "#oppty_chatter_df[['COLUMN NAME']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatter_url = \"URL\".format(sf = native['instance'])\n",
    "\n",
    "bad_record_list = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for idx, val in oppty_chatter_df.iterrows():\n",
    "    \n",
    "    hyperlink_dict = {\n",
    "        'hyperlink_1': ['URL', ' Monetization Strategy Team.', 1]\n",
    "    }\n",
    "    \n",
    "    oppty_id = df.loc[idx, 'column']\n",
    "    data_for_payload = {\n",
    "        'mention_1': oppty_chatter_df.loc[idx, 'column']\n",
    "        , 'body_1': oppty_chatter_df.loc[idx, 'column']\n",
    "        , 'mention_2': oppty_chatter_df.loc[idx, 'column']\n",
    "        , 'hyperlink': hyperlink_dict\n",
    "    }\n",
    "    \n",
    "    chatter_json_payload = create_oppty_chatter_payload_hyperlink(oppty_id\n",
    "                                                                 , data_for_payload\n",
    "                                                                 , hyperlink_key = 'hyperlink'\n",
    "                                                                 , inline_hyperlink = True)\n",
    "    \n",
    "    #print(chatter_json_payload)\n",
    "\n",
    "    #\"\"\"\n",
    "    try:\n",
    "        chatter_response = requests.request(\"POST\", chatter_url, data = chatter_json_payload, headers = headers)\n",
    "        chatter_response.json()\n",
    "        \n",
    "        chatter_response.raise_for_status()\n",
    "        print('Chatter Message Post Successful for index: ' + str(idx))\n",
    "        \n",
    "    except Exception as msg:\n",
    "        print('Issue with POST request for index: ' + str(idx) + ' with error: ' + str(msg))\n",
    "        bad_record_list.append(idx)\n",
    "    \n",
    "end_time = time.time()\n",
    "print('Total Time Taken: ' + str(end_time - start_time) + ' seconds')\n",
    "    \n",
    "bad_record_df = oppty_chatter_df.iloc[bad_record_list, :]\n",
    "#\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
