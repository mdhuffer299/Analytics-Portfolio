{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Score Data\n",
    "#### Note: If you are not retraining the model, you will just need to execute the Score Notebook on the refreshed data.  If you are retraining the model, ensure that the version number is updated prior to serializing the model to disk to version models over time for comparisions.\n",
    "#### If rescoring data, refresh the following tables prior to rescore: \n",
    "#### After rescoring data refresh the following tables prior to creating target list: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all required modules including Oracle connection and Data Processing Functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T18:25:55.834778Z",
     "start_time": "2019-11-13T18:25:46.430980Z"
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import cx_Oracle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import statsmodels.api as sm\n",
    "import joblib\n",
    "import datetime\n",
    "import shap\n",
    "\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from statistics import mean\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from itertools import zip_longest\n",
    "\n",
    "# Update path to where function file resides\n",
    "if os.name == 'nt':\n",
    "    state = !cd\n",
    "    \n",
    "    # Load DB Connection File from Windows Machine\n",
    "    os.chdir(r'directory name')\n",
    "    from db_connection import oracle_connection\n",
    "    \n",
    "    # Load function file from Windows Machine\n",
    "    os.chdir(r'directory name')\n",
    "    from general_functions import *\n",
    "elif os.name == 'posix':\n",
    "    state = !pwd\n",
    "    \n",
    "    # Load DB Connection File from Mac Machine\n",
    "    os.chdir('directry name')\n",
    "    from db_connection import oracle_connection\n",
    "    \n",
    "    # Load function file from Mac Machine\n",
    "    os.chdir('directory name')\n",
    "    from general_functions import *\n",
    "else:\n",
    "    print('No OS!')\n",
    "\n",
    "#Change directory back to working Jupyter Notebook Directory after importing connection module\n",
    "os.chdir(state[0])\n",
    "\n",
    "todays_date = datetime.date.today().strftime('%Y%m%d')\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DB Connection String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.name == 'nt':\n",
    "    # Update path to where config file resides\n",
    "    db_creds = os.path.expanduser('~') + 'directory name'\n",
    "    creds = oracle_connection(db_creds)\n",
    "\n",
    "    url = creds['host'] + \":\" + creds['port'] + \"/\" + creds['database']\n",
    "\n",
    "    db = cx_Oracle.connect(creds['user'], creds['password'], url)\n",
    "\n",
    "    cursor = db.cursor()\n",
    "elif os.name == 'posix':\n",
    "    # Update path to where config file resides\n",
    "    db_creds = os.path.expanduser('~') + 'directory name'\n",
    "    creds = oracle_connection(db_creds)\n",
    "\n",
    "    url = creds['host'] + \":\" + creds['port'] + \"/\" + creds['database']\n",
    "\n",
    "    db = cx_Oracle.connect(creds['user'], creds['password'], url, encoding = 'UTF-8')\n",
    "    cursor = db.cursor()\n",
    "else:\n",
    "    print('No OS!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Accounts without Target product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T18:28:07.550780Z",
     "start_time": "2019-11-13T18:25:57.599078Z"
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "score_df = pd.read_sql(query, cursor.connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T18:28:07.762471Z",
     "start_time": "2019-11-13T18:28:07.555317Z"
    }
   },
   "outputs": [],
   "source": [
    "score_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T18:28:07.780413Z",
     "start_time": "2019-11-13T18:28:07.770870Z"
    }
   },
   "outputs": [],
   "source": [
    "score_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T18:28:15.130890Z",
     "start_time": "2019-11-13T18:28:14.981117Z"
    }
   },
   "outputs": [],
   "source": [
    "lr_model_fit = joblib.load('./Model/dtr_logistic_regression_v2.0.pkl')\n",
    "rf_model_fit = joblib.load('./Model/dtr_random_forest_v2.0.pkl')\n",
    "xgb_model_fit = joblib.load('./Model/dtr_xgboost_v2.0.pkl')\n",
    "encoder = joblib.load('./Model/encoder_v1.0.pkl')\n",
    "scaler_fit = joblib.load('./Model/scaler_v1.0.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T18:28:16.581891Z",
     "start_time": "2019-11-13T18:28:16.127628Z"
    }
   },
   "outputs": [],
   "source": [
    "LABEL_VAL = 'target column'\n",
    "\n",
    "score_df_tr = score_df.copy()\n",
    "score_df_tr = score_df_tr.drop(['column'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace all missing values with 'None' or 0 depending on the Data Type of column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T18:28:18.458105Z",
     "start_time": "2019-11-13T18:28:17.830971Z"
    }
   },
   "outputs": [],
   "source": [
    "dtype_dict_value = replace_values(score_df_tr, char_value = 'Unknown')\n",
    "score_df_tr.fillna(value = dtype_dict_value, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert all columns that are Factor Levels or Flag columns into Category data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_col_list = list(score_df_tr.select_dtypes(include = ['object']).columns)\n",
    "encode_df = pd.DataFrame(encoder.transform(score_df_tr[string_col_list]).toarray(), columns = encoder.get_feature_names(string_col_list))\n",
    "\n",
    "encode_col_dict = create_encode_col_dict(score_df_tr, encoder)\n",
    "\n",
    "score_df_tr = score_df_tr.merge(encode_df, left_index = True, right_index = True)\n",
    "score_df_tr = score_df_tr.drop(string_col_list,  axis = 1)\n",
    "\n",
    "score_df_tr = score_df_tr.rename(columns = encode_col_dict)\n",
    "\n",
    "score_df_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T18:28:26.475087Z",
     "start_time": "2019-11-13T18:28:26.456274Z"
    }
   },
   "source": [
    "## Drop Highly Correlated Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r dtr_unique_corr_cols\n",
    "\n",
    "score_df_tr = score_df_tr.drop(dtr_unique_corr_cols, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize columns using Scaler Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T18:28:32.833899Z",
     "start_time": "2019-11-13T18:28:31.929324Z"
    }
   },
   "outputs": [],
   "source": [
    "label = score_df_tr[LABEL_VAL]\n",
    "column_headers = score_df_tr.drop(LABEL_VAL, axis = 1).columns\n",
    "score_df_std = pd.DataFrame(scaler_fit.transform(score_df_tr.drop(LABEL_VAL, axis = 1)), columns = column_headers)\n",
    "score_df_std = pd.DataFrame(label).merge(score_df_std, left_index = True, right_index = True)\n",
    "\n",
    "score_df_std.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return column order for XGB Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r xgb_col_order\n",
    "\n",
    "xgb_features = score_df_std.reindex(columns = xgb_col_order)\n",
    "\n",
    "xgb_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the final dataframe with scored labels and respective probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T18:28:45.485204Z",
     "start_time": "2019-11-13T18:28:39.313559Z"
    }
   },
   "outputs": [],
   "source": [
    "features = score_df_std.drop([LABEL_VAL], axis = 1)\n",
    "score_df_std['PRED_LABEL_LR'] = lr_model_fit.predict(features)\n",
    "score_df_std['PRED_LABEL_RF'] = rf_model_fit.predict(features)\n",
    "score_df_std['PRED_LABEL_XGB'] = xgb_model_fit.predict(xgb_features)\n",
    "\n",
    "score_prob_df = pd.DataFrame()\n",
    "\n",
    "score_prob_df['LR_PROB_ZERO'] = lr_model_fit.predict_proba(features)[: ,0]\n",
    "score_prob_df['LR_PROB_ONE'] = lr_model_fit.predict_proba(features)[: ,1]\n",
    "score_prob_df['RF_PROB_ZERO'] = rf_model_fit.predict_proba(features)[: ,0]\n",
    "score_prob_df['RF_PROB_ONE'] = rf_model_fit.predict_proba(features)[: ,1]\n",
    "score_prob_df['XGB_PROB_ZERO'] = xgb_model_fit.predict_proba(xgb_features)[:, 0]\n",
    "score_prob_df['XGB_PROB_ONE'] = xgb_model_fit.predict_proba(xgb_features)[:, 1]\n",
    "\n",
    "score_df_std = score_df_std.join(score_prob_df)\n",
    "\n",
    "score_df_std['AVG_PROB_ZERO'] = (score_df_std['LR_PROB_ZERO'] + score_df_std['RF_PROB_ZERO'])/2\n",
    "score_df_std['AVG_PROB_ONE'] = (score_df_std['LR_PROB_ONE'] + score_df_std['RF_PROB_ONE'])/2\n",
    "\n",
    "score_df_std[['B2C_CMMRC_FLG'\n",
    "              , 'PRED_LABEL_LR', 'LR_PROB_ZERO', 'LR_PROB_ONE'\n",
    "              , 'PRED_LABEL_RF', 'RF_PROB_ZERO', 'RF_PROB_ONE'\n",
    "              , 'AVG_PROB_ZERO', 'AVG_PROB_ONE', 'PRED_LABEL_XGB'\n",
    "              , 'XGB_PROB_ZERO', 'XGB_PROB_ONE']].head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retirn list of column names from Random Forest Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T18:28:51.957902Z",
     "start_time": "2019-11-13T18:28:51.893068Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_import_df = pd.DataFrame()\n",
    "rf_import_df['FEATURE_NAME'], rf_import_df['FEATURE_IMPORTANCE'] = features.columns, rf_model_fit.feature_importances_\n",
    "rf_import_cols = list(rf_import_df.sort_values(by = ['FEATURE_IMPORTANCE'], axis = 0, ascending = False).head(20)['FEATURE_NAME'])\n",
    "rf_import_df.sort_values(by = ['FEATURE_IMPORTANCE'], axis = 0, ascending = False).head(20)\n",
    "\n",
    "%store -r dtr_lr_import_feats\n",
    "\n",
    "rf_import_cols.extend(dtr_lr_import_feats)\n",
    "\n",
    "import_cols = list(set(rf_import_cols))\n",
    "\n",
    "exclude_list = ['COLUMN NAME']\n",
    "import_cols = [col for col in import_cols if col not in exclude_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score_df_std['PRED_LABEL_LR'].value_counts(), \"\\n\"\n",
    "      , score_df_std['PRED_LABEL_RF'].value_counts(), \"\\n\"\n",
    "      , score_df_std['PRED_LABEL_XGB'].value_counts(), \"\\n\"\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate Final Dataframe with Summary SHAP value output column that has friendly names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T18:28:59.885182Z",
     "start_time": "2019-11-13T18:28:58.332915Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "friendly_col_names_sql = \"\"\"\n",
    "                            SELECT\n",
    "                                DB_COLUMN_NM\n",
    "                                , FRIENDLY_NM\n",
    "                            FROM MHUFFER.LM_FRIENDLY_NAMES\n",
    "\"\"\"\n",
    "\n",
    "friendly_col_df = pd.read_sql(friendly_col_names_sql, cursor.connection)\n",
    "\n",
    "friendly_column_name = dict(zip(friendly_col_df.DB_COLUMN_NM, friendly_col_df.FRIENDLY_NM))\n",
    "#print(friendly_column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_summary = shap_summary(xgb_model_fit, xgb_features, friendly_column_dict = friendly_column_name, standardized_df = True, scaler_fit = scaler_fit)\n",
    "\n",
    "score_df_std['SUMMARY_OUTPUT'] = output_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_final_df = score_df.merge(score_df_std, left_index = True, right_index = True)\n",
    "check_cols = score_final_df.columns\n",
    "\n",
    "keep_cols = ['ACCT_ID', 'PRED_LABEL_LR', 'LR_PROB_ZERO', 'LR_PROB_ONE', 'PRED_LABEL_RF', 'RF_PROB_ZERO', 'RF_PROB_ONE', 'AVG_PROB_ZERO', 'AVG_PROB_ONE', 'PRED_LABEL_XGB', 'XGB_PROB_ZERO', 'XGB_PROB_ONE', 'SUMMARY_OUTPUT']\n",
    "\n",
    "score_final_df.drop(score_final_df.columns.difference(keep_cols), axis = 1, inplace = True)\n",
    "\n",
    "score_final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write scored data back to Oracle Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T14:12:14.161805Z",
     "start_time": "2019-09-05T14:12:09.904368Z"
    }
   },
   "outputs": [],
   "source": [
    "drop_table_sql = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(drop_table_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T18:32:33.079865Z",
     "start_time": "2019-11-13T18:32:32.884266Z"
    }
   },
   "outputs": [],
   "source": [
    "create_table_sql = \"\"\"\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(create_table_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T18:33:00.003449Z",
     "start_time": "2019-11-13T18:32:33.083321Z"
    }
   },
   "outputs": [],
   "source": [
    "records = [tuple(x) for x in score_final_df.values]\n",
    "cursor.executemany('''''', records)\n",
    "db.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
